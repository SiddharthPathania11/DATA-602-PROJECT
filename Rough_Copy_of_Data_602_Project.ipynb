{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiddharthPathania11/DATA-602-PROJECT/blob/main/Rough_Copy_of_Data_602_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROJECT 602\n",
        "## PRINCIPLES OF DATA SCIENCE\n"
      ],
      "metadata": {
        "id": "Ye8ZDw7Zhy2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUBMITTED TO: DR. Charles Forgy\n",
        "SUBMITTED BY\n",
        "\n",
        "*  Aneesh Krishna Rao Chepuri 121324382\n",
        "*  Omkarnath Thakur 121335685\n",
        "*  Ruthvick K Kandrala 121305206\n",
        "*  Siddharth Pathania 121291592\n",
        "\n"
      ],
      "metadata": {
        "id": "oFyukZaMibBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INTRODUCTION\n",
        "\n",
        "\n",
        " We will be working on American Express - Default Predictions dataset from kaggle via api. We\n",
        " will be analyzing different attributes of the given datasets, performing prescriptive, descriptive\n",
        " and predictive analysis to get the likelihood that the customer using the credit card will default\n",
        " or not.We will implement different machine learning and deep learning models to ensure a best\n",
        " fit model with highest accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "j0sr4a0PjCiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEPS INVOLVED(Tentative):\n",
        "\n",
        " 1. DATA CLEANING:-\n",
        "\n",
        "  * Remove Duplicate\n",
        "  * Error Handling\n",
        "  * Data Validation\n",
        " 2. DATA PREPROCESSING:-\n",
        "  * Filling NaN values and Missing Values\n",
        "  * Format check all every single column.\n",
        "  * Dimensionality Reduction techniques like PCA , T-SNE\n",
        "  * Feature importance\n",
        "  * Feature selection\n",
        " 3. EDA\n",
        " 4. Feature Engineering\n",
        " 5. Train different ML and DL Models.\n",
        " 6. Hyperparameter tuning for the best model fit and minimizing the the required loss function.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xifGPPvxjQv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Location of the dataset api :- kaggle competitions download -c amex-default-prediction\n",
        "\n",
        "#### Link for the dataset :- https://www.kaggle.com/competitions/amex-default-prediction/data"
      ],
      "metadata": {
        "id": "Mrl5mOoRj6JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project part II\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. The column names of the feature(s) you plan to use for the final project, and the target outcome you plan to determine using these features.\n",
        "\n",
        "  The dataset contains features regarding customer profile at each statement date. Features are anonymized and the fall in these categories :-\n",
        "\n",
        "  Delinquency (D_*), Spend (S_*), Payment (P_*), Balance (B_*), and Risk (R_*).\n",
        "  We will be using these features for the final project. Along the process we will drop features which are less relevant, thus keeping only important features.\n",
        "\n",
        "  The target is a binary variable, and it tells us whether a customer has paid or has not paid his due amount in 120 days after their latest statement, therefore we will predict this target variable using the features.\n",
        "\n",
        "\n",
        "2.  Your initial plan for any feature engineering (e.g. adding a Boolean for weekend vs. weekday or taking the ratio of two other features).\n",
        "\n",
        "  For the American Express Default Prediction, the initial approach will involve:\n",
        "\n",
        "\n",
        "\n",
        "*   Categorical columns:\n",
        "  * Apply one-hot encoding for nominal categories and  for cardinal data.\n",
        "  * Use frequency or target encoding with smoothing to capture relationships with the target variables.\n",
        "\n",
        "\n",
        "*   Numerical columns:\n",
        "  * Handle missing data using KNN imputation or Missing Value imputation using ML models.\n",
        "  * Scale features and create meaningful interactions (ratios, differences).\n",
        "  * Apply PCA  or  LDA to reduce dimensionality.\n",
        "\n",
        "\n",
        "*   Time-based columns (date columns):\n",
        "  * Create lagged features and rolling windows to capture customer behavior over time.\n",
        "\n",
        "\n",
        "*   Aggregation:\n",
        "  * Aggregate transaction-level data to customer-level using statistics like mean, sum, and standard deviation.\n",
        "\n",
        "\n",
        "*   Feature selection:\n",
        "  * Apply techniques to retain the most important features for model refinement.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. If you plan to use any sort of method for dealing with imbalanced data, detail your overall approach (e.g. \"will oversample class XYZ\", or \"will create synthetic data using SMOTE\"). If you do not intend to use any techniques to deal with imbalanced data, explain why it is not necessary (e.g. \"all features are at worst imbalanced 7:3\").\n",
        "\n",
        "  Based on the initial data analysis we saw that % data for people who default is less as compared to people who have paid. To handle this situation we are planning to use different methods, and will see which one works best for us. Also we know that credit defaults are typically rare events, often comprising less %age of the dataset.\n",
        "\n",
        "  * **Oversampling the minority class:** We'll use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic examples of the minority class. This will help balance the dataset without simply duplicating existing data points.\n",
        "\n",
        "  * **Undersampling the majority class:** We'll carefully undersample the majority class (non-defaulters) using methods like Random Under Sampling or Tomek links to remove some of the majority instances while maintaining the dataset's quality.\n",
        "\n",
        "  * **Ensemble methods:** We'll use ensemble techniques like BalancedRandomForestClassifier or EasyEnsembleClassifier, which are designed to handle imbalanced datasets effectively.\n",
        "\n",
        "  * **Class-weight adjustment:** Assign higher weights to the minority class in algorithms like Random Forest, XGBoost, or Logistic Regression making the model more sensitive to defaults.\n",
        "\n",
        "  Here are some initial techniques that we are planning to use. Along the process if we find more useful techniques we will use them as well, and then we will carefully monitor performance metrics to see which methods are more effective to give us more accurate results.\n",
        "\n"
      ],
      "metadata": {
        "id": "_xbcZg6igBzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "1Mr7z6wYh9JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "mZAsTMtmgBM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bae93c-4f8f-4908-f85c-8245c9ef282f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elRjT4PaAiOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing all the necessary libraries"
      ],
      "metadata": {
        "id": "85CGloZ_AjGf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z10FcTYXhyBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the dataset"
      ],
      "metadata": {
        "id": "NsN6wye0Aeyp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0KLuxeJbhyDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDtGCqmGIJDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EkBZCoorhyFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # This will prompt you to upload kaggle.json\n",
        "\n",
        "# Create Kaggle directory and move the json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set the permissions for the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset from Kaggle competition\n",
        "!kaggle competitions download -c amex-default-prediction\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip -q amex-default-prediction.zip\n",
        "\n",
        "# Load the data into pandas DataFrame\n",
        "import pandas as pd\n",
        "train_data = pd.read_csv('train_data.csv')\n",
        "train_labels = pd.read_csv('train_labels.csv')\n",
        "\n",
        "# Merge train data with labels\n",
        "data = train_data.merge(train_labels, on=\"customer_ID\")\n",
        "\n",
        "# Check the first few rows of the data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "p9AvXp29QXP5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "7adf1cb0-5b75-4879-92c2-8e0c81c93c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-545b1c17-bd9c-4870-b684-9ec195f01923\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-545b1c17-bd9c-4870-b684-9ec195f01923\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"sid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVcXUKEfVwPL",
        "outputId": "bd3aa3bd-d2dc-4f05-d715-fd87b46024a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s2Q79GtXRH6w"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygvX8h8NVNJ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}